{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
    "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas \n",
    "#%pip install matplotlib\n",
    "#%pip install scikit-learn\n",
    "import sklearn as sk\n",
    "from sklearn import datasets\n",
    "# add commented pip installation lines for packages used as shown above for ease of testing\n",
    "# the line should be of the format %pip install PACKAGE_NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DO NOT CHANGE** the filepath variable\n",
    "##### Instead, create a folder named 'data' in your current working directory and \n",
    "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALL** Code for machine learning and dataset analysis should be entered below. \n",
    "##### Ensure that your code is clear and readable.\n",
    "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can have as many cells as you want for code\n",
    "import pandas as pd\n",
    "filepath = \"./data/catB_train.parquet\" \n",
    "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file\n",
    "\n",
    "df = pd.read_parquet(filepath)\n",
    "\n",
    "cols_df1 = df.columns[:8]  \n",
    "cols_df2 = df.columns[8:18]  \n",
    "cols_df3 = df.columns[18:24] \n",
    "cols_df4 = df.columns[24:34]  \n",
    "cols_df5 = df.columns[34:41] \n",
    "cols_df6 = df.columns[41:158]  \n",
    "cols_df7 = df.columns[158:303] #will be removed\n",
    "cols_df8 = df.columns[303:] \n",
    "\n",
    "df1 = df[cols_df1]\n",
    "df2 = df[cols_df2]\n",
    "df3 = df[cols_df3]\n",
    "df4 = df[cols_df4]\n",
    "df5 = df[cols_df5]\n",
    "df6 = df[cols_df6]\n",
    "df7 = df[cols_df7]\n",
    "df8 = df[cols_df8]\n",
    "\n",
    "df1 = df1.drop(columns=df1.columns[:2])\n",
    "df3 = df3[df3.columns[2:3]]\n",
    "df3 = df3[df3.columns[2:3]]\n",
    "df4 = df4.iloc[:, [0, 1, 2, 8, 9]]\n",
    "df5 = df5.drop(columns = ['recency_cancel'])\n",
    "df6 = df6.drop(columns=df6.columns[96:103])\n",
    "df6 = df6.astype('float')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining columns in cat 6 based on ape, sumins and prempaid\n",
    "\n",
    "ape_av = df6.columns[0:30].to_list() + df6.columns[89:96].to_list()\n",
    "df6['av_ape'] = df6[ape_av].mean(axis=1)\n",
    "df6 = df6.drop(columns=ape_av)\n",
    "\n",
    "sumins_av = df6.columns[0:29].to_list() + df6.columns[59:66].to_list()\n",
    "df6['av_sumins'] = df6[sumins_av].mean(axis=1)\n",
    "df6 = df6.drop(columns=sumins_av)\n",
    "\n",
    "prempaid_av = df6.columns[0:37]\n",
    "df6['av_prempaid'] = df6[prempaid_av].mean(axis=1)\n",
    "df6 = df6.drop(columns=prempaid_av)\n",
    "\n",
    "df = pd.concat([df1, df2, df3, df4, df5, df6, df8], axis=1)\n",
    "df.columns.get_loc(\"f_purchase_lh\")\n",
    "df = df.drop_duplicates()\n",
    "df = df.drop('min_occ_date', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting NaN and None\n",
    "\n",
    "null = pd.isnull(df)\n",
    "null.head()\n",
    "\n",
    "# Filling null values\n",
    "\n",
    "df.fillna(0)\n",
    "\n",
    "#Count the total number of missing values\n",
    "pd.isnull(df).sum().sum()\n",
    "\n",
    "#dropped irrelevant/unhelpful rows\n",
    "for x in df.index:\n",
    "    if df.loc[x, \"ctrycode_desc\"] == \"Not Applicable\":\n",
    "        df.drop(x, inplace=True)\n",
    "        \n",
    "for x in df.index:\n",
    "    if df.loc[x, \"annual_income_est\"] == '':\n",
    "        df.replace(to_replace = None, value = 0)\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "#knn for income\n",
    "income = df['annual_income_est']\n",
    "income.value_counts()\n",
    "df[\"annual_income_est\"][13:14]\n",
    "from sklearn.impute import KNNImputer\n",
    "#imputer_knn = KNNImputer(missing_values=None)\n",
    "#imputer_knn.fit_transform(df[\"annual_income_est\"])\n",
    "\n",
    "#knn for recency lapse\n",
    "imputer_knn = KNNImputer(missing_values=None)\n",
    "#imputer_knn.fit_transform(df[\"recency_lapse\"]) #tried to impute numerical values but knnimputer needs it to be a 2d array\n",
    "\n",
    "csv_output = \"./data/catB_train.csv\" \n",
    "df.to_csv(csv_output, index = False)\n",
    "\n",
    "#df = df.interpolate(method='pad')\n",
    "df[\"f_purchase_lh\"] = df[\"f_purchase_lh\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04165033484771666"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#correlation between target and annual income\n",
    "#df[\"av_ape\"].corr(df[\"f_purchase_lh\"]) #all the numeric comparisons gave nan output\n",
    "#df = df.apply([pd.to_numeric]) #we tried to make all the categorical numeric but also did not work\n",
    "\n",
    "\n",
    "df[\"av_ape\"].corr(df[\"f_purchase_lh\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "#converting categorical to numerical\n",
    "dummy_date = '1970-01-01'\n",
    "df['cltdob_fix'] = df['cltdob_fix'].replace('None', dummy_date)\n",
    "df['cltdob_fix'] = pd.to_datetime(df['cltdob_fix']).dt.year % 100\n",
    "\n",
    "df['cltsex_fix'] = df['cltsex_fix'].map({'Male': 0, 'Female': 1})\n",
    "\n",
    "df['hh_size_est'] = df['hh_size_est'].map({'1': 1, '2': 2, '3': 3, '4': 4, '>4': 5})\n",
    "\n",
    "df['annual_income_est'] = df['cltsex_fix'].map({'annual_income_est_E.BELOW30K': 1, 'annual_income_est_D.30K-60K ': 2, 'annual_income_est_C.60K-100K': 3, 'annual_income_est_B.100K-200K': 4, 'annual_income_est_A.ABOVE200K': 5})\n",
    "\n",
    "\n",
    "\n",
    "#one hot encoding\n",
    "df = pd.get_dummies(df, columns=['ctrycode_desc', 'clttype', 'stat_flag'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m feature_importances \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfeature_importances_\n\u001b[1;32m     12\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m---> 14\u001b[0m feature_importance_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFeature\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mImportance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_importances\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m feature_importance_df \u001b[38;5;241m=\u001b[39m feature_importance_df\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImportance\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(feature_importance_df)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/frame.py:767\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    761\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    762\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    763\u001b[0m     )\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "#decision tree classifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "y = df[\"f_purchase_lh\"]\n",
    "X = df.drop(columns = ['f_purchase_lh'])\n",
    "model.fit(X, y)\n",
    "\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "feature_names = df.columns\n",
    "\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df)\n",
    "decision_tree_model.feature_importances_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imblearn in /Users/sehunful/Library/Python/3.9/lib/python/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /Users/sehunful/Library/Python/3.9/lib/python/site-packages (from imblearn) (0.12.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/sehunful/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn->imblearn) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/sehunful/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn->imblearn) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/sehunful/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn->imblearn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/sehunful/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn->imblearn) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/sehunful/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn->imblearn) (1.4.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below is **NOT** to be removed\n",
    "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n",
    "##### It is recommended to test the function out prior to submission\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
    "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
    "    '''DO NOT REMOVE THIS FUNCTION.\n",
    "\n",
    "The function accepts a dataframe as input and return an iterable (list)\n",
    "of binary classes as output.\n",
    "\n",
    "The function should be coded to test on hidden data\n",
    "and should include any preprocessing functions needed for your model to perform. \n",
    "    \n",
    "All relevant code MUST be included in this function.'''\n",
    "    result = [] \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell to check testing_hidden_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# This cell should output a list of predictions.\n",
    "test_df = pd.read_parquet(filepath)\n",
    "test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
    "print(testing_hidden_data(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

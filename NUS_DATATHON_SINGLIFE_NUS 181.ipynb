{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
    "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 23:41:31.554414: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (/Users/melaniegan/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomOverSampler, SMOTE\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_class_weight\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SGD\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/__init__.py:52\u001b[0m\n\u001b[1;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m         combine,\n\u001b[1;32m     54\u001b[0m         ensemble,\n\u001b[1;32m     55\u001b[0m         exceptions,\n\u001b[1;32m     56\u001b[0m         metrics,\n\u001b[1;32m     57\u001b[0m         over_sampling,\n\u001b[1;32m     58\u001b[0m         pipeline,\n\u001b[1;32m     59\u001b[0m         tensorflow,\n\u001b[1;32m     60\u001b[0m         under_sampling,\n\u001b[1;32m     61\u001b[0m         utils,\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/combine/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/combine/_smote_enn.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/base.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_sampling_strategy, check_target_type\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArraysTransformer\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSamplerMixin\u001b[39;00m(BaseEstimator, metaclass\u001b[38;5;241m=\u001b[39mABCMeta):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/utils/_param_validation.py:908\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_valid_param  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m--> 908\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    909\u001b[0m     HasMethods,\n\u001b[1;32m    910\u001b[0m     Hidden,\n\u001b[1;32m    911\u001b[0m     Interval,\n\u001b[1;32m    912\u001b[0m     Options,\n\u001b[1;32m    913\u001b[0m     StrOptions,\n\u001b[1;32m    914\u001b[0m     _ArrayLikes,\n\u001b[1;32m    915\u001b[0m     _Booleans,\n\u001b[1;32m    916\u001b[0m     _Callables,\n\u001b[1;32m    917\u001b[0m     _CVObjects,\n\u001b[1;32m    918\u001b[0m     _InstancesOf,\n\u001b[1;32m    919\u001b[0m     _IterablesNotString,\n\u001b[1;32m    920\u001b[0m     _MissingValues,\n\u001b[1;32m    921\u001b[0m     _NoneConstraint,\n\u001b[1;32m    922\u001b[0m     _PandasNAConstraint,\n\u001b[1;32m    923\u001b[0m     _RandomStates,\n\u001b[1;32m    924\u001b[0m     _SparseMatrices,\n\u001b[1;32m    925\u001b[0m     _VerboseHelper,\n\u001b[1;32m    926\u001b[0m     make_constraint,\n\u001b[1;32m    927\u001b[0m     validate_params,\n\u001b[1;32m    928\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (/Users/melaniegan/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py)"
     ]
    }
   ],
   "source": [
    "#%pip install pandas \n",
    "#%pip install matplotlib\n",
    "#%pip install scikit-learn\n",
    "#%pip install seaborn\n",
    "#%pip install tensorflow\n",
    "import sklearn as sk\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from sklearn.utils import compute_class_weight\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# add commented pip installation lines for packages used as shown above for ease of testing\n",
    "# the line should be of the format %pip install PACKAGE_NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DO NOT CHANGE** the filepath variable\n",
    "##### Instead, create a folder named 'data' in your current working directory and \n",
    "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALL** Code for machine learning and dataset analysis should be entered below. \n",
    "##### Ensure that your code is clear and readable.\n",
    "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./data/catB_train.parquet\" \n",
    "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file\n",
    "\n",
    "df = pd.read_parquet(filepath)\n",
    "\n",
    "cols_df1 = df.columns[:8]  \n",
    "cols_df2 = df.columns[8:18]  \n",
    "cols_df3 = df.columns[18:24] \n",
    "cols_df4 = df.columns[24:34]  \n",
    "cols_df5 = df.columns[34:41] \n",
    "cols_df6 = df.columns[41:158]  \n",
    "cols_df7 = df.columns[158:303] #will be removed\n",
    "cols_df8 = df.columns[303:] \n",
    "\n",
    "df1 = df[cols_df1]\n",
    "df2 = df[cols_df2]\n",
    "df3 = df[cols_df3]\n",
    "df4 = df[cols_df4]\n",
    "df5 = df[cols_df5]\n",
    "df6 = df[cols_df6]\n",
    "df7 = df[cols_df7]\n",
    "df8 = df[cols_df8]\n",
    "\n",
    "df1 = df1.drop(columns=df1.columns[:2])\n",
    "df3 = df3[df3.columns[2:3]]\n",
    "df3 = df3[df3.columns[2:3]]\n",
    "df4 = df4.iloc[:, [0, 1, 2, 8, 9]]\n",
    "df5 = df5.drop(columns = ['recency_cancel'])\n",
    "df6 = df6.drop(columns=df6.columns[96:103])\n",
    "df6 = df6.astype('float')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining columns in cat 6 based on ape, sumins and prempaid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ape_av = df6.columns[0:30].to_list() + df6.columns[89:96].to_list()\n",
    "df6['av_ape'] = df6[ape_av].mean(axis=1)\n",
    "df6 = df6.drop(columns=ape_av)\n",
    "\n",
    "sumins_av = df6.columns[0:29].to_list() + df6.columns[59:66].to_list()\n",
    "df6['av_sumins'] = df6[sumins_av].mean(axis=1)\n",
    "df6 = df6.drop(columns=sumins_av)\n",
    "\n",
    "prempaid_av = df6.columns[0:37]\n",
    "df6['av_prempaid'] = df6[prempaid_av].mean(axis=1)\n",
    "df6 = df6.drop(columns=prempaid_av)\n",
    "\n",
    "df = pd.concat([df1, df2, df3, df4, df5, df6, df8], axis=1)\n",
    "df.columns.get_loc(\"f_purchase_lh\")\n",
    "df = df.drop_duplicates()\n",
    "df = df.drop('min_occ_date', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting NaN and None\n",
    "\n",
    "null = pd.isnull(df)\n",
    "null.head()\n",
    "\n",
    "# Filling null values\n",
    "\n",
    "df.fillna(0)\n",
    "df['recency_lapse'] = df['recency_lapse'].fillna(0)\n",
    "\n",
    "#Count the total number of missing values\n",
    "pd.isnull(df).sum().sum()\n",
    "\n",
    "#dropped irrelevant/unhelpful rows\n",
    "for x in df.index:\n",
    "    if df.loc[x, \"ctrycode_desc\"] == \"Not Applicable\":\n",
    "        df.drop(x, inplace=True)\n",
    "        \n",
    "df = df.dropna(subset=[\"annual_income_est\"])\n",
    "\n",
    "\n",
    "csv_output = \"./data/catB_train.csv\" \n",
    "df.to_csv(csv_output, index = False)\n",
    "\n",
    "#df = df.interpolate(method='pad')\n",
    "df[\"f_purchase_lh\"] = df[\"f_purchase_lh\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation between target and av_ape\n",
    "\n",
    "\n",
    "print(df[\"av_ape\"].corr(df[\"f_purchase_lh\"]))\n",
    "print(df[\"av_sumins\"].corr(df[\"f_purchase_lh\"]))\n",
    "print(df[\"av_prempaid\"].corr(df[\"f_purchase_lh\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting categorical variables to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "dummy_date = '1970-01-01'\n",
    "df['cltdob_fix'] = df['cltdob_fix'].replace('None', dummy_date)\n",
    "df['cltdob_fix'] = pd.to_datetime(df['cltdob_fix']).dt.year % 100\n",
    "\n",
    "df['cltsex_fix'] = df['cltsex_fix'].map({'Male': 0, 'Female': 1})\n",
    "df[\"cltsex_fix\"] = df[\"cltsex_fix\"].fillna(0)\n",
    "\n",
    "df['hh_size_est'] = df['hh_size_est'].map({'1': 1, '2': 2, '3': 3, '4': 4, '>4': 5})\n",
    "df[\"hh_size_est\"] = df[\"hh_size_est\"].fillna(0)\n",
    "\n",
    "df['annual_income_est'] = df['annual_income_est'].map({'E.BELOW30K': 1, 'D.30K-60K': 2, 'C.60K-100K': 3, 'B.100K-200K': 4, 'A.ABOVE200K': 5})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['ctrycode_desc', 'clttype', 'stat_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.isnull(), cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"flg_substandard\"]) \n",
    "df = df.drop(columns = ['tot_cancel_pols']) #more than 50% data is missing\n",
    "sns.heatmap(df.isnull(), cbar=False)\n",
    "plt.show() #all NA values have been imputed or removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating predictors and response variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"f_purchase_lh\"]\n",
    "X = df.drop(columns = ['f_purchase_lh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree classifier (remove?)\n",
    "model = DecisionTreeClassifier()\n",
    "y = df[\"f_purchase_lh\"]\n",
    "X = df.drop(columns = ['f_purchase_lh'])\n",
    "model.fit(X, y)\n",
    "\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "feature_names = X.columns\n",
    "#print(\"Length of feature_names:\", len(feature_names))\n",
    "#print(\"Length of feature_importances:\", len(feature_importances))\n",
    "\n",
    "#(feature_names.shape())\n",
    "\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "#print(len(feature_names))\n",
    "#print(feature_importance_df[:20])\n",
    "model.feature_importances_\n",
    "\n",
    "n = 15  # Number of top features to select\n",
    "important_indices = feature_importance_df.index[:n]\n",
    "#print(important_indices)\n",
    "X_new = X.iloc[:, important_indices]\n",
    "X = X_new\n",
    "print(X_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x = df.drop(columns = 'f_purchase_lh')\n",
    "all_x = np.asarray(all_x).astype('float32')\n",
    "#all_x = np.asarray(X).astype('float32')\n",
    "all_y = df['f_purchase_lh']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data for SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(all_x, all_y, test_size = 0.1, random_state = 181)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X and y are your feature matrix and target variable respectively\n",
    "\n",
    "# For upsampling with SMOTE to generate synthetic samples\n",
    "sampler = SMOTE(random_state = 181, k_neighbors = 3, sampling_strategy = 0.8)\n",
    "\n",
    "# Resample the dataset\n",
    "x_resampled, y_resampled = sampler.fit_resample(x_train, y_train)\n",
    "#print(type(X_resampled))\n",
    "\n",
    "print(pd.DataFrame(x_resampled))\n",
    "print(sum(y_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating fully connected model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classWeight = compute_class_weight('balanced', classes = np.unique(y_train), y = y_train) \n",
    "#classWeight = dict(enumerate(classWeight))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(33 * 33, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(33, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(loss= tf.keras.losses.binary_crossentropy,\n",
    "\n",
    "                optimizer = 'adam',\n",
    "\n",
    "                metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model based on the SMOTE sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_resampled, y_resampled, batch_size = 10, epochs = 100, verbose = 1, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "confusion_matrix(y_test, y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.keras')\n",
    "model = tf.keras.models.load_model('model.keras')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    return tf.keras.models.load_model('model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below is **NOT** to be removed\n",
    "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n",
    "##### It is recommended to test the function out prior to submission\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
    "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_hidden_data(hidden_data: pd.DataFrame) -> list: \n",
    "    '''DO NOT REMOVE THIS FUNCTION.\n",
    "\n",
    "The function accepts a dataframe as input and return an iterable (list)\n",
    "of binary classes as output.\n",
    "\n",
    "The function should be coded to test on hidden data\n",
    "and should include any preprocessing functions needed for your model to perform. \n",
    "    \n",
    "All relevant code MUST be included in this function.'''\n",
    "    \n",
    "    cols_df1 = df.columns[:8]  \n",
    "    cols_df2 = df.columns[8:18]  \n",
    "    cols_df3 = df.columns[18:24] \n",
    "    cols_df4 = df.columns[24:34]  \n",
    "    cols_df5 = df.columns[34:41] \n",
    "    cols_df6 = df.columns[41:158]  \n",
    "    cols_df7 = df.columns[158:303] \n",
    "    cols_df8 = df.columns[303:] \n",
    "\n",
    "    df1 = df[cols_df1]\n",
    "    df2 = df[cols_df2]\n",
    "    df3 = df[cols_df3]\n",
    "    df4 = df[cols_df4]\n",
    "    df5 = df[cols_df5]\n",
    "    df6 = df[cols_df6]\n",
    "    df7 = df[cols_df7]\n",
    "    df8 = df[cols_df8]\n",
    "\n",
    "    df1 = df1.drop(columns=df1.columns[:2])\n",
    "    df3 = df3[df3.columns[2:3]]\n",
    "    df3 = df3[df3.columns[2:3]]\n",
    "    df4 = df4.iloc[:, [0, 1, 2, 8, 9]]\n",
    "    df5 = df5.drop(columns = ['recency_cancel'])\n",
    "    df6 = df6.drop(columns=df6.columns[96:103])\n",
    "    df6 = df6.astype('float')\n",
    "\n",
    "    ape_av = df6.columns[0:30].to_list() + df6.columns[89:96].to_list()\n",
    "    df6['av_ape'] = df6[ape_av].mean(axis=1)\n",
    "    df6 = df6.drop(columns=ape_av)\n",
    "\n",
    "    sumins_av = df6.columns[0:29].to_list() + df6.columns[59:66].to_list()\n",
    "    df6['av_sumins'] = df6[sumins_av].mean(axis=1)\n",
    "    df6 = df6.drop(columns=sumins_av)\n",
    "\n",
    "    prempaid_av = df6.columns[0:37]\n",
    "    df6['av_prempaid'] = df6[prempaid_av].mean(axis=1)\n",
    "    df6 = df6.drop(columns=prempaid_av)\n",
    "\n",
    "    df = pd.concat([df1, df2, df3, df4, df5, df6, df8], axis=1)\n",
    "    df.columns.get_loc(\"f_purchase_lh\")\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.drop('min_occ_date', axis=1)\n",
    "\n",
    "    # Detecting NaN and None\n",
    "\n",
    "    null = pd.isnull(df)\n",
    "    null.head()\n",
    "\n",
    "    # Filling null values\n",
    "\n",
    "    df.fillna(0)\n",
    "    df['recency_lapse'] = df['recency_lapse'].fillna(0)\n",
    "\n",
    "    #Count the total number of missing values\n",
    "    pd.isnull(df).sum().sum()\n",
    "\n",
    "    #dropped irrelevant/unhelpful rows\n",
    "    for x in df.index:\n",
    "        if df.loc[x, \"ctrycode_desc\"] == \"Not Applicable\":\n",
    "            df.drop(x, inplace=True)\n",
    "        \n",
    "    df = df.dropna(subset=[\"annual_income_est\"])\n",
    "\n",
    "\n",
    "    csv_output = \"./data/catB_train.csv\" \n",
    "    df.to_csv(csv_output, index = False)\n",
    "\n",
    "    #df = df.interpolate(method='pad')\n",
    "    df[\"f_purchase_lh\"] = df[\"f_purchase_lh\"].fillna(0)\n",
    "\n",
    "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    dummy_date = '1970-01-01'\n",
    "    df['cltdob_fix'] = df['cltdob_fix'].replace('None', dummy_date)\n",
    "    df['cltdob_fix'] = pd.to_datetime(df['cltdob_fix']).dt.year % 100\n",
    "\n",
    "    df['cltsex_fix'] = df['cltsex_fix'].map({'Male': 0, 'Female': 1})\n",
    "    df[\"cltsex_fix\"] = df[\"cltsex_fix\"].fillna(0)\n",
    "\n",
    "    df['hh_size_est'] = df['hh_size_est'].map({'1': 1, '2': 2, '3': 3, '4': 4, '>4': 5})\n",
    "    df[\"hh_size_est\"] = df[\"hh_size_est\"].fillna(0)\n",
    "\n",
    "    df['annual_income_est'] = df['annual_income_est'].map({'E.BELOW30K': 1, 'D.30K-60K': 2, 'C.60K-100K': 3, 'B.100K-200K': 4, 'A.ABOVE200K': 5})\n",
    "\n",
    "    df = pd.get_dummies(df, columns=['ctrycode_desc', 'clttype', 'stat_flag'])\n",
    "    \n",
    "    df = df.dropna(subset=[\"flg_substandard\"]) \n",
    "    df = df.drop(columns = ['tot_cancel_pols'])\n",
    "    \n",
    "    y = df[\"f_purchase_lh\"]\n",
    "    X = df.drop(columns = ['f_purchase_lh'])\n",
    "    \n",
    "     \n",
    "    #decision tree classifier \n",
    "    model = DecisionTreeClassifier()\n",
    "    y = df[\"f_purchase_lh\"]\n",
    "    X = df.drop(columns = ['f_purchase_lh'])\n",
    "    model.fit(X, y)\n",
    "\n",
    "    feature_importances = model.feature_importances_\n",
    "\n",
    "    feature_names = X.columns\n",
    "\n",
    "\n",
    "    feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    model.feature_importances_\n",
    "\n",
    "    n = 15  # Number of top features to select\n",
    "    important_indices = feature_importance_df.index[:n]\n",
    "    \n",
    "    X_new = X.iloc[:, important_indices]\n",
    "    X = X_new\n",
    "\n",
    "    all_x = df.drop(columns = 'f_purchase_lh')\n",
    "    all_x = np.asarray(all_x).astype('float32')\n",
    "    #all_x = np.asarray(X).astype('float32')\n",
    "    all_y = df['f_purchase_lh']\n",
    "\n",
    "    # Assuming X and y are your feature matrix and target variable respectively\n",
    "\n",
    "    # For upsampling with SMOTE to generate synthetic samples\n",
    "    sampler = SMOTE(random_state = 181, k_neighbors = 3, sampling_strategy = 0.8)\n",
    "\n",
    "    # Resample the dataset\n",
    "    x_resampled, y_resampled = sampler.fit_resample(x_train, y_train)\n",
    "    #print(type(X_resampled))\n",
    "    \n",
    "    model = tf.keras.models.load_model('model.keras')\n",
    "    y_pred = model.predict(x_test)\n",
    "    result = [1 if i > 0.5 else 0 for i in y_pred]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell to check testing_hidden_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should output a list of predictions.\n",
    "test_df = pd.read_parquet(filepath)\n",
    "test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
    "print(testing_hidden_data(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
